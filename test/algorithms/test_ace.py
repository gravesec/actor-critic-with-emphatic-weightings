import unittest
import gym
import numpy as np
from tqdm import tqdm
import scipy.stats as st
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from src.algorithms.ace import BinaryACE
from src.algorithms.fhat import BinaryFHat
from src.algorithms.tdc import BinaryTDC, BinaryGQ
from src.function_approximation.tile_coder import TileCoder
from evaluate_policies import evaluate_policy


class ACETests(unittest.TestCase):

    def test_binary_ace_on_policy(self):
        env = gym.make('MountainCar-v0').unwrapped  # Get the underlying environment object to bypass the built-in timestep limit.
        env.seed(992390476)  # Seed generated by: np.random.randint(2**31 - 1)
        rng = env.np_random

        alpha_a = .05
        alpha_c = .2
        alpha_c2 = .01
        lambda_c = .9

        tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
        actor = BinaryACE(env.action_space.n, tc.total_num_tiles, alpha_a / tc.num_active_features)
        critic = BinaryTDC(tc.total_num_tiles, alpha_c / tc.num_active_features, alpha_c2 / tc.num_active_features, lambda_c)

        gamma = 1.
        num_episodes = 10
        total_reward = np.full(num_episodes, np.nan)
        for episode_num in tqdm(range(num_episodes)):
            g = 0.
            f_t = 1.
            critic.z *= 0.  # reset traces
            indices_t = tc.encode(env.reset())
            for t in range(1000):
                a_t = rng.choice(env.action_space.n, p=actor.pi(indices_t))  # Select action.
                s_tp1, r_tp1, terminal, _ = env.step(a_t)  # Interact with environment.
                indices_tp1 = tc.encode(s_tp1)
                delta_t = r_tp1 + gamma * (0 if terminal else critic.estimate(indices_tp1)) - critic.estimate(indices_t)
                critic.learn(delta_t, indices_t, gamma, indices_tp1, gamma, rho_t=1.)
                actor.learn(indices_t, a_t, delta_t, f_t, rho_t=1)  # Update actor.
                indices_t = indices_tp1
                f_t *= gamma
                g += r_tp1
                if terminal:
                    break
            total_reward[episode_num] = g
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.plot(total_reward, label='TD Error AC')
        plt.legend(loc='lower right')
        plt.title('Mountain Car (On-policy)')
        plt.xlabel('Episode Number')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('td_error_ac_on_policy.png')

        self.assertGreater(total_reward[-1], -200)

    def test_aa_binary_ace_on_policy(self):
        env = gym.make('MountainCar-v0').unwrapped  # Get the underlying environment object to bypass the built-in timestep limit.
        env.seed(317850564)  # Seed generated by: np.random.randint(2**31 - 1)
        rng = env.np_random

        alpha_a = .05
        alpha_c = .2
        alpha_c2 = .01
        lambda_c = .9

        tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
        actor = BinaryACE(env.action_space.n, tc.total_num_tiles, alpha_a / tc.num_active_features)
        critic = BinaryGQ(env.action_space.n, tc.total_num_tiles, alpha_c / tc.num_active_features, alpha_c2 / tc.num_active_features, lambda_c)

        gamma = 1.
        num_episodes = 10
        total_reward = np.full(num_episodes, np.nan)
        for episode_num in tqdm(range(num_episodes)):
            g = 0.
            f_t = 1.
            critic.z *= 0.  # reset traces
            indices_t = tc.encode(env.reset())
            for t in range(1000):
                a_t = rng.choice(env.action_space.n, p=actor.pi(indices_t))  # Select action.
                s_tp1, r_tp1, terminal, _ = env.step(a_t)  # Interact with environment.
                indices_tp1 = tc.encode(s_tp1)
                critic.learn(indices_t, a_t, 1, gamma, r_tp1, indices_tp1, actor.pi(indices_tp1), 0 if terminal else gamma)
                q_t = critic.estimate(indices_t)
                actor.all_actions_learn(indices_t, q_t, f_t)
                indices_t = indices_tp1
                f_t *= gamma
                g += r_tp1
                if terminal:
                    break
            total_reward[episode_num] = g
        fig = plt.figure()
        ax = fig.add_subplot(111)
        ax.plot(total_reward, label='All Actions AC')
        plt.legend(loc='lower right')
        plt.title('Mountain Car (On-policy)')
        plt.xlabel('Episode Number')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('all_actions_ac_on_policy.png')

        self.assertGreater(total_reward[-1], -250)

    def test_binary_ace_off_policy(self):
        env = gym.make('MountainCar-v0').unwrapped
        env.seed(1202470738)
        rng = env.np_random

        # ACE parameters:
        alpha_a = .0005
        alpha_c = .1
        alpha_w = .0005
        lambda_c = 0.
        eta = 1.
        i = lambda g=1: 1.  # Uniform interest.

        # OffPAC parameters:
        # alpha_a = .01
        # alpha_c = .01
        # alpha_w = .00005
        # lambda_c = .4
        # eta = 0.
        # i = lambda g=1: 1.  # Uniform interest.

        gamma = 1.
        num_runs = 1
        num_timesteps = 20000
        evaluation_interval = 1000
        num_evaluation_runs = 10
        rewards = np.zeros((num_runs, num_timesteps // evaluation_interval + 1, num_evaluation_runs))
        for run_num in range(num_runs):
            tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
            actor = BinaryACE(env.action_space.n, tc.total_num_tiles, alpha_a / tc.num_active_features)
            critic = BinaryTDC(tc.total_num_tiles, alpha_c / tc.num_active_features, alpha_w / tc.num_active_features, lambda_c)

            mu = np.ones(env.action_space.n) / env.action_space.n  # Uniform random policy.
            gamma_t = 0.
            f_t = 1.
            rho_tm1 = 1.
            indices_t = tc.encode(env.reset())
            for t in tqdm(range(num_timesteps)):
                if t % evaluation_interval == 0:
                    rewards[run_num, t // evaluation_interval] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))
                a_t = rng.choice(env.action_space.n, p=mu)
                s_tp1, r_tp1, terminal, _ = env.step(a_t)
                gamma_tp1 = 0. if terminal else gamma
                indices_tp1 = tc.encode(s_tp1)
                rho_t = actor.pi(indices_t)[a_t] / mu[a_t]
                delta_t = r_tp1 + gamma_tp1 * critic.estimate(indices_tp1) - critic.estimate(indices_t)
                critic.learn(delta_t, indices_t, gamma_t, indices_tp1, gamma_tp1, rho_t)
                i_t = i(gamma_t)
                f_t = rho_tm1 * gamma_t * f_t + i_t
                m_t = (1 - eta) * i_t + eta * f_t
                actor.learn(indices_t, a_t, delta_t, m_t, rho_t)
                gamma_t = gamma_tp1
                indices_t = indices_tp1
                rho_tm1 = rho_t

                if terminal:
                    gamma_t = 0.
                    f_t = 1.
                    rho_tm1 = 1.
                    s_tp1 = env.reset()

            rewards[run_num, -1] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))

        # Plot results:
        mean_eval_rewards = np.mean(rewards, axis=2)
        var_eval_rewards = np.var(rewards, axis=2)
        mean_rewards = np.mean(mean_eval_rewards, axis=0)
        sem_rewards = np.sqrt(np.sum(var_eval_rewards / num_evaluation_runs, axis=0)) / num_runs
        fig = plt.figure()
        ax = fig.add_subplot(111)
        x = np.array([evaluation_interval*i for i in range(num_timesteps // evaluation_interval + 1)])
        confs = sem_rewards * st.t.ppf((1.0 + 0.95) / 2, num_evaluation_runs - 1)
        label = f'$\\alpha_a$:{alpha_a}, $\\alpha_c$:{alpha_c}, $\\alpha_w$:{alpha_w}, $\\lambda_c$:{lambda_c}, $\\eta$:{eta}{"(OffPAC)" if eta==0. else ""}'
        ax.errorbar(x, mean_rewards, yerr=[confs, confs], label=label)
        plt.legend(loc='lower right')
        plt.title('Mountain Car (off-policy)')
        plt.xlabel('Timesteps')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('binary_ace_off_policy.png')
        self.assertGreater(mean_rewards[-1], -200)

    def test_all_actions_binary_ace_off_policy(self):
        env = gym.make('MountainCar-v0').unwrapped
        env.seed(1202470738)
        rng = env.np_random

        # ACE parameters:
        # alpha_a = .00005
        # alpha_c = .01
        # alpha_w = .00005
        # lambda_c = 0.
        # eta = 1.
        # i = lambda g=1: 1.  # Uniform interest.
        # num_timesteps = 20000

        # OffPAC parameters:
        alpha_a = .01
        alpha_c = .01
        alpha_w = .00005
        lambda_c = .4
        eta = 0.
        i = lambda g=1: 1.  # Uniform interest.
        num_timesteps = 20000

        gamma = 1.
        num_runs = 1
        evaluation_interval = 1000
        num_evaluation_runs = 10
        rewards = np.zeros((num_runs, num_timesteps // evaluation_interval + 1, num_evaluation_runs))
        for run_num in range(num_runs):
            tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
            actor = BinaryACE(env.action_space.n, tc.total_num_tiles)
            critic = BinaryGQ(env.action_space.n, tc.total_num_tiles, alpha_c / tc.num_active_features, alpha_w / tc.num_active_features, lambda_c)

            mu = np.ones(env.action_space.n) / env.action_space.n  # Uniform random policy.
            gamma_t = 0.
            f_t = 1.
            rho_tm1 = 1.
            indices_t = tc.encode(env.reset())
            for t in tqdm(range(num_timesteps)):
                if t % evaluation_interval == 0:
                    rewards[run_num, t // evaluation_interval] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))
                a_t = rng.choice(env.action_space.n, p=mu)
                s_tp1, r_tp1, terminal, _ = env.step(a_t)
                gamma_tp1 = 0. if terminal else gamma
                indices_tp1 = tc.encode(s_tp1)
                rho_t = actor.pi(indices_t)[a_t] / mu[a_t]
                delta_t = r_tp1 + gamma_tp1 * critic.estimate(indices_tp1) - critic.estimate(indices_t)
                critic.learn(indices_t, a_t, rho_t, gamma_t, r_tp1, indices_tp1, actor.pi(indices_tp1), gamma_tp1)
                i_t = i(gamma_t)
                f_t = rho_tm1 * gamma_t * f_t + i_t
                actor.all_actions_learn(critic.estimate(indices_t), indices_t, i_t, eta, alpha_a / tc.num_active_features, f_t)
                gamma_t = gamma_tp1
                indices_t = indices_tp1
                rho_tm1 = rho_t

                if terminal:
                    gamma_t = 0.
                    f_t = 1.
                    rho_tm1 = 1.
                    s_tp1 = env.reset()

            rewards[run_num, -1] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))

        # Plot results:
        mean_eval_rewards = np.mean(rewards, axis=2)
        var_eval_rewards = np.var(rewards, axis=2)
        mean_rewards = np.mean(mean_eval_rewards, axis=0)
        sem_rewards = np.sqrt(np.sum(var_eval_rewards / num_evaluation_runs, axis=0)) / num_runs
        fig = plt.figure()
        ax = fig.add_subplot(111)
        x = np.array([evaluation_interval*i for i in range(num_timesteps // evaluation_interval + 1)])
        confs = sem_rewards * st.t.ppf((1.0 + 0.95) / 2, num_evaluation_runs - 1)
        label = f'$\\alpha_a$:{alpha_a}, $\\alpha_c$:{alpha_c}, $\\alpha_w$:{alpha_w}, $\\lambda_c$:{lambda_c}, $\\eta$:{eta}{"(OffPAC)" if eta==0. else ""}'
        ax.errorbar(x, mean_rewards, yerr=[confs, confs], label=label)
        plt.legend(loc='lower right')
        plt.title('Mountain Car (off-policy)')
        plt.xlabel('Timesteps')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('aa_binary_ace_off_policy.png')
        self.assertGreater(mean_rewards[-1], -200)

    def test_low_variance_binary_ace_off_policy(self):
        env = gym.make('MountainCar-v0').unwrapped
        env.seed(1202470738)
        rng = env.np_random

        # ACE parameters:
        alpha_a = .001
        alpha_c = .01
        alpha_w = .0005
        lambda_c = 0.
        eta = 1.
        i = lambda g=1: 1.  # Uniform interest.

        # OffPAC parameters:
        # alpha_a = .01
        # alpha_c = .01
        # alpha_w = .00005
        # lambda_c = .4
        # eta = 0.
        # i = lambda g=1: 1.  # Uniform interest.

        gamma = 1.
        num_runs = 1
        num_timesteps = 20000
        evaluation_interval = 1000
        num_evaluation_runs = 10
        rewards = np.zeros((num_runs, num_timesteps // evaluation_interval + 1, num_evaluation_runs))
        for run_num in range(num_runs):
            tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
            actor = BinaryACE(env.action_space.n, tc.total_num_tiles, alpha_a / tc.num_active_features)
            critic = BinaryTDC(tc.total_num_tiles, alpha_c / tc.num_active_features, alpha_w / tc.num_active_features, lambda_c)
            fhat = BinaryFHat(tc.total_num_tiles, .1)

            mu = np.ones(env.action_space.n) / env.action_space.n  # Uniform random policy.
            gamma_t = 0.
            f_t = 1.
            rho_tm1 = 1.
            indices_t = tc.encode(env.reset())
            for t in tqdm(range(num_timesteps)):
                if t % evaluation_interval == 0:
                    rewards[run_num, t // evaluation_interval] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))
                a_t = rng.choice(env.action_space.n, p=mu)
                s_tp1, r_tp1, terminal, _ = env.step(a_t)
                gamma_tp1 = 0. if terminal else gamma
                indices_tp1 = tc.encode(s_tp1)
                rho_t = actor.pi(indices_t)[a_t] / mu[a_t]
                delta_t = r_tp1 + gamma_tp1 * critic.estimate(indices_tp1) - critic.estimate(indices_t)
                critic.learn(delta_t, indices_t, gamma_t, indices_tp1, gamma_tp1, rho_t)
                i_t = i(gamma_t)
                f_t = fhat.estimate(indices_t)
                m_t = (1 - eta) * i_t + eta * f_t
                actor.learn(indices_t, a_t, delta_t, m_t, rho_t)
                gamma_t = gamma_tp1
                indices_t = indices_tp1
                rho_tm1 = rho_t

                if terminal:
                    gamma_t = 0.
                    f_t = 1.
                    rho_tm1 = 1.
                    s_tp1 = env.reset()

            rewards[run_num, -1] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))

        # Plot results:
        mean_eval_rewards = np.mean(rewards, axis=2)
        var_eval_rewards = np.var(rewards, axis=2)
        mean_rewards = np.mean(mean_eval_rewards, axis=0)
        sem_rewards = np.sqrt(np.sum(var_eval_rewards / num_evaluation_runs, axis=0)) / num_runs
        fig = plt.figure()
        ax = fig.add_subplot(111)
        x = np.array([evaluation_interval*i for i in range(num_timesteps // evaluation_interval + 1)])
        confs = sem_rewards * st.t.ppf((1.0 + 0.95) / 2, num_evaluation_runs - 1)
        label = f'$\\alpha_a$:{alpha_a}, $\\alpha_c$:{alpha_c}, $\\alpha_w$:{alpha_w}, $\\lambda_c$:{lambda_c}, $\\eta$:{eta}{"(OffPAC)" if eta==0. else ""}'
        ax.errorbar(x, mean_rewards, yerr=[confs, confs], label=label)
        plt.legend(loc='lower right')
        plt.title('Mountain Car (off-policy)')
        plt.xlabel('Timesteps')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('low_variance_binary_ace_off_policy.png')
        self.assertGreater(mean_rewards[-1], -200)


if __name__ == '__main__':
    unittest.main()
