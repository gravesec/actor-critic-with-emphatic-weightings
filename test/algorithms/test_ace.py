import unittest
import gym
import numpy as np
from tqdm import tqdm
from src.algorithms.ace import LinearACE, BinaryACE
from src.algorithms.totd import LinearTOTD, BinaryTOTD
from src.function_approximation.tile_coder import TileCoder
from visualize import plot_learned_policy, plot_learned_value_function
from evaluate_policies import evaluate_policy


class ACETests(unittest.TestCase):

    def test_learn_one(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([2], dtype=np.intp)
        a_t = 0
        delta_t = 1  # action was better than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1, eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertGreater(after[a_t], before[a_t])

    def test_learn_two(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([0, 1, 2], dtype=np.intp)
        a_t = 0
        delta_t = -1  # action was worse than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1., eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertLess(after[a_t], before[a_t])

    def test_binary_ace_on_policy(self):
        env = gym.make('MountainCar-v0').env  # Get the underlying environment object to bypass the built-in timestep limit.
        env.seed(992390476)  # Seed generated by: np.random.randint(2**31 - 1)
        num_timesteps = 10000
        num_features = 4096
        gamma = 1.
        tc = TileCoder(env.observation_space.low, env.observation_space.high, 8, 8, num_features, True)
        actor = BinaryACE(env.action_space.n, num_features)
        critic = BinaryTOTD(num_features, .0625, 0.)

        indices_t = tc.indices(env.reset())
        gamma_t = 0.
        for t in tqdm(range(num_timesteps)):
            a_t = np.random.choice(env.action_space.n, p=actor.pi(indices_t))  # Select action.
            s_tp1, r_tp1, terminal, _ = env.step(a_t)  # Interact with environment.
            if terminal:
                gamma_tp1 = 0.
                s_tp1 = env.reset()
            else:
                gamma_tp1 = gamma
            indices_tp1 = tc.indices(s_tp1)

            v_t = critic.estimate(indices_t)
            v_tp1 = critic.estimate(indices_tp1)
            delta_t = r_tp1 + gamma_tp1 * v_tp1 - v_t  # Compute TD error.

            critic.learn(delta_t, v_t, indices_t, gamma_t, v_tp1)  # Update critic.
            actor.learn(gamma_t, i_t=1., eta_t=1 if gamma_t == 0. else 0., alpha_t=.001, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)  # Update actor.
            env.render()
            indices_t = indices_tp1
            gamma_t = gamma_tp1

        plot_learned_policy(tc, actor)
        plot_learned_value_function(tc, critic)
        env.reset()
        g_t = evaluate_policy(actor, tc, env)
        self.assertGreater(g_t, -250)


if __name__ == '__main__':
    unittest.main()
