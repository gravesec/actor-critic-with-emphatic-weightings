import unittest
import gym
import numpy as np
from tqdm import tqdm
import scipy.stats as st
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from src.algorithms.ace import LinearACE, BinaryACE
from src.algorithms.totd import LinearTOTD, BinaryTOTD
from src.algorithms.tdc import BinaryTDC
from src.function_approximation.tile_coder import TileCoder
from evaluate_policies import evaluate_policy


class ACETests(unittest.TestCase):

    def test_learn_one(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([2], dtype=np.intp)
        a_t = 0
        delta_t = 1  # action was better than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1, eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertGreater(after[a_t], before[a_t])

    def test_learn_two(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([0, 1, 2], dtype=np.intp)
        a_t = 0
        delta_t = -1  # action was worse than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1., eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertLess(after[a_t], before[a_t])

    def test_binary_ace_on_policy(self):
        env = gym.make('MountainCar-v0').unwrapped  # Get the underlying environment object to bypass the built-in timestep limit.
        env.seed(992390476)  # Seed generated by: np.random.randint(2**31 - 1)
        num_timesteps = 2500
        gamma = 1.
        tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)
        actor = BinaryACE(env.action_space.n, tc.total_num_tiles)
        critic = BinaryTOTD(tc.total_num_tiles, .1/tc.num_active_features, .9)

        indices_t = tc.encode(env.reset())
        gamma_t = 0.
        for t in tqdm(range(num_timesteps)):
            a_t = np.random.choice(env.action_space.n, p=actor.pi(indices_t))  # Select action.
            s_tp1, r_tp1, terminal, _ = env.step(a_t)  # Interact with environment.
            if terminal:
                gamma_tp1 = 0.
                s_tp1 = env.reset()
            else:
                gamma_tp1 = gamma
            indices_tp1 = tc.encode(s_tp1)

            v_t = critic.estimate(indices_t)
            v_tp1 = critic.estimate(indices_tp1)
            delta_t = r_tp1 + gamma_tp1 * v_tp1 - v_t  # Compute TD error.

            critic.learn(delta_t, v_t, indices_t, gamma_t, v_tp1)  # Update critic.
            actor.learn(gamma_t, i_t=1., eta_t=1 if gamma_t == 0. else 0., alpha_t=.001, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)  # Update actor.
            # env.render()
            indices_t = indices_tp1
            gamma_t = gamma_tp1

        # plot_learned_policy(actor, tc)
        # plot_learned_value_function(critic, tc)
        env.reset()
        g_t = evaluate_policy(actor, tc, env)
        self.assertGreater(g_t, -250)

    def test_binary_ace_off_policy(self):
        num_runs = 2
        num_timesteps = 5000
        evaluation_interval = 1000
        num_evaluation_runs = 1
        rewards = np.zeros((num_runs, num_timesteps // evaluation_interval + 1, num_evaluation_runs))

        alpha_a = .001
        alpha_c = .05
        alpha_w = .0001
        lambda_c = 0.
        eta = 0.

        env = gym.make('MountainCar-v0').unwrapped
        env.seed(1202670738)
        rng = env.np_random

        tc = TileCoder(np.array([env.observation_space.low, env.observation_space.high]).T, [5, 5], 8, True)

        for run_num in range(num_runs):
            actor = BinaryACE(env.action_space.n, tc.total_num_tiles)
            critic = BinaryTDC(tc.total_num_tiles, alpha_c, alpha_w, lambda_c)

            indices_t = tc.encode(env.reset())
            gamma_t = 0.
            for t in tqdm(range(num_timesteps)):
                if t % evaluation_interval == 0:
                    rewards[run_num, t // evaluation_interval] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=1000) for _ in range(num_evaluation_runs))

                a_t = rng.choice(env.action_space.n)
                s_tp1, r_tp1, terminal, _ = env.step(a_t)
                if terminal:
                    s_tp1 = env.reset()
                    gamma_tp1 = 0.
                else:
                    gamma_tp1 = 1.
                indices_tp1 = tc.encode(s_tp1)

                pi_t = actor.pi(indices_t)
                mu_t = np.ones(env.action_space.n) / env.action_space.n
                rho_t = pi_t[a_t] / mu_t[a_t]

                delta_t = r_tp1 + gamma_tp1 * critic.estimate(indices_tp1) - critic.estimate(indices_t)
                critic.learn(delta_t, indices_t, gamma_t, indices_tp1, gamma_tp1, rho_t)
                actor.learn(gamma_t, 1., eta, alpha_a, rho_t, delta_t, indices_t, a_t)

                gamma_t = gamma_tp1
                indices_t = indices_tp1
            rewards[run_num, -1] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=5000) for _ in range(num_evaluation_runs))

        # Plot results:
        mean_eval_rewards = np.mean(rewards, axis=2)
        var_eval_rewards = np.var(rewards, axis=2)

        mean_rewards = np.mean(mean_eval_rewards, axis=0)
        sem_rewards = np.sqrt(np.sum(var_eval_rewards / num_evaluation_runs, axis=0)) / num_runs

        fig = plt.figure()
        ax = fig.add_subplot(111)
        x = np.array([evaluation_interval*i for i in range(num_timesteps // evaluation_interval + 1)])
        confs = sem_rewards * st.t.ppf((1.0 + 0.95) / 2, num_evaluation_runs - 1)
        label = '$\\alpha_a$:{}, $\\alpha_c$:{}, $\\alpha_w$:{}, $\\lambda_c$:{}, $\\eta$:{}{}'.format(alpha_a, alpha_c, alpha_w, lambda_c, eta, '(OffPAC)' if eta == 0. else '')
        ax.errorbar(x, mean_rewards, yerr=[confs, confs], label=label)
        plt.legend(loc='lower right')
        plt.title('Mountain Car')
        plt.xlabel('Timesteps')
        plt.ylabel('Total Reward')
        plt.ylim(-1000, 0)
        plt.savefig('total_rewards.png')
        # plt.show()

        self.assertGreater(mean_rewards[-1], -250)


if __name__ == '__main__':
    unittest.main()
