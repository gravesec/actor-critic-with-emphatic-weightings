import unittest
import gym
import numpy as np
from tqdm import tqdm
import scipy.stats as st
import matplotlib.pyplot as plt
from joblib import Parallel, delayed
from src.algorithms.ace import LinearACE, BinaryACE
from src.algorithms.totd import LinearTOTD, BinaryTOTD
from src.algorithms.tdc import BinaryTDC
from src.function_approximation.tile_coder import TileCoder
from visualize import plot_learned_policy, plot_learned_value_function
from evaluate_policies import evaluate_policy


class ACETests(unittest.TestCase):

    def test_learn_one(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([2], dtype=np.intp)
        a_t = 0
        delta_t = 1  # action was better than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1, eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertGreater(after[a_t], before[a_t])

    def test_learn_two(self):
        ace = BinaryACE(2, 3)
        indices_t = np.array([0, 1, 2], dtype=np.intp)
        a_t = 0
        delta_t = -1  # action was worse than expected
        before = ace.pi(indices_t)
        ace.learn(gamma_t=.9, i_t=1., eta_t=1., alpha_t=.1, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)
        after = ace.pi(indices_t)
        # If the action taken turned out to be better than expected, one would expect the probability of taking it in the future to increase:
        self.assertLess(after[a_t], before[a_t])

    def test_binary_ace_on_policy(self):
        env = gym.make('MountainCar-v0').env  # Get the underlying environment object to bypass the built-in timestep limit.
        env.seed(992390476)  # Seed generated by: np.random.randint(2**31 - 1)
        num_timesteps = 10000
        num_features = 4096
        gamma = 1.
        tc = TileCoder(env.observation_space.low, env.observation_space.high, 8, 8, num_features, True)
        actor = BinaryACE(env.action_space.n, num_features)
        critic = BinaryTOTD(num_features, .0625, 0.)

        indices_t = tc.indices(env.reset())
        gamma_t = 0.
        for t in tqdm(range(num_timesteps)):
            a_t = np.random.choice(env.action_space.n, p=actor.pi(indices_t))  # Select action.
            s_tp1, r_tp1, terminal, _ = env.step(a_t)  # Interact with environment.
            if terminal:
                gamma_tp1 = 0.
                s_tp1 = env.reset()
            else:
                gamma_tp1 = gamma
            indices_tp1 = tc.indices(s_tp1)

            v_t = critic.estimate(indices_t)
            v_tp1 = critic.estimate(indices_tp1)
            delta_t = r_tp1 + gamma_tp1 * v_tp1 - v_t  # Compute TD error.

            critic.learn(delta_t, v_t, indices_t, gamma_t, v_tp1)  # Update critic.
            actor.learn(gamma_t, i_t=1., eta_t=1 if gamma_t == 0. else 0., alpha_t=.001, rho_t=1., delta_t=delta_t, indices_t=indices_t, a_t=a_t)  # Update actor.
            env.render()
            indices_t = indices_tp1
            gamma_t = gamma_tp1

        # plot_learned_policy(actor, tc)
        # plot_learned_value_function(critic, tc)
        env.reset()
        g_t = evaluate_policy(actor, tc, env)
        self.assertGreater(g_t, -250)

    def test_binary_ace_off_policy(self):
        num_timesteps = 50000
        evaluation_interval = 1000
        num_evaluation_runs = 5
        rewards = np.zeros((num_timesteps // evaluation_interval + 1, num_evaluation_runs))

        alpha_a = .001
        alpha_c = .05
        alpha_w = .0001
        lambda_c = 0.
        eta = 0.

        env = gym.make('MountainCar-v0').env
        # env.seed(1202670738)
        rng = env.np_random

        num_features = 100000
        num_tiles = 9
        num_tilings = 9
        tc = TileCoder(env.observation_space.low, env.observation_space.high, num_tiles, num_tilings, num_features, True)
        actor = BinaryACE(env.action_space.n, num_features)
        critic = BinaryTDC(num_features, alpha_c, alpha_w, lambda_c)

        indices_t = tc.indices(env.reset())
        gamma_t = 0.
        for t in tqdm(range(num_timesteps)):
            if t % evaluation_interval == 0:
                rewards[t // evaluation_interval] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=5000) for _ in range(num_evaluation_runs))

            a_t = rng.choice(env.action_space.n)
            s_tp1, r_tp1, terminal, _ = env.step(a_t)
            if terminal:
                s_tp1 = env.reset()
                gamma_tp1 = 0.
            else:
                gamma_tp1 = 1.
            indices_tp1 = tc.indices(s_tp1)

            pi_t = actor.pi(indices_t)
            mu_t = np.ones(env.action_space.n) / env.action_space.n
            rho_t = pi_t[a_t] / mu_t[a_t]

            delta_t = r_tp1 + gamma_tp1 * critic.estimate(indices_tp1) - critic.estimate(indices_t)
            critic.learn(delta_t, indices_t, gamma_t, indices_tp1, gamma_tp1, rho_t)
            actor.learn(gamma_t, 1., eta, alpha_a, rho_t, delta_t, indices_t, a_t)

            gamma_t = gamma_tp1
            indices_t = indices_tp1
        rewards[-1] = Parallel(n_jobs=-1)(delayed(evaluate_policy)(actor, tc, num_timesteps=5000) for _ in range(num_evaluation_runs))

        # Plot results:
        mean_rewards = np.mean(rewards, axis=1)
        sem_rewards = st.sem(rewards, axis=1)

        fig = plt.figure()
        ax = fig.add_subplot(111)
        x = np.array([evaluation_interval*i for i in range(num_timesteps // evaluation_interval + 1)])
        confs = sem_rewards * st.t.ppf((1.0 + 0.95) / 2, num_evaluation_runs - 1)
        label = '$\\alpha_a$:{}, $\\alpha_c$:{}, $\\alpha_w$:{}, $\\lambda_c$:{}, $\\eta$:{}'.format(alpha_a, alpha_c, alpha_w, lambda_c, eta)
        ax.errorbar(x, mean_rewards, yerr=[confs, confs], label=label)
        plt.legend(loc='lower right')
        plt.title('Mountain Car')
        plt.xlabel('Timesteps')
        plt.ylabel('Total Reward')
        plt.ylim(-5000, 0)
        plt.savefig('total_rewards.png')
        # plt.show()

        self.assertGreater(mean_rewards[-1], -250)


if __name__ == '__main__':
    unittest.main()
